DATABASE_URL=postgresql+asyncpg://user:password@postgres:5432/scheduler
REDIS_URL=redis://redis:6379
LLM_PROVIDER=local
# Ollama runs on host (see README / make ollama-serve). Backend in Docker reaches it via:
OLLAMA_BASE_URL=http://host.docker.internal:11434
OPENAI_API_KEY=
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini
OLLAMA_MODEL=llama3:8b
# Local Ollama: first request can take 10â€“30s while model loads (cold start)
LLM_PARSE_TIMEOUT_SECONDS=60
LLM_HOSTED_TIMEOUT_SECONDS=10
LLM_MAX_RETRIES=2
DEV_MODE=true
# Date-only and "today/tomorrow" logic use this timezone (e.g. America/Toronto)
ORG_TIMEZONE=America/Toronto

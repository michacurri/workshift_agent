DATABASE_URL=postgresql+asyncpg://user:password@postgres:5432/scheduler
REDIS_URL=redis://redis:6379
LLM_PROVIDER=local
# Comma-separated origin allowlist (no wildcards in production)
CORS_ALLOW_ORIGINS=http://localhost:5173,http://localhost:5174
# Ollama runs on host (see README / make ollama-serve). Backend in Docker reaches it via:
OLLAMA_BASE_URL=http://host.docker.internal:11434

# Hosted LLM (production): set LLM_PROVIDER=hosted and configure vendor + credentials below.
# Default vendor is anthropic (Claude).
HOSTED_LLM_VENDOR=anthropic
ANTHROPIC_API_KEY=
ANTHROPIC_BASE_URL=https://api.anthropic.com
ANTHROPIC_MODEL=claude-3-5-sonnet-latest
ANTHROPIC_VERSION=2023-06-01

# Optional OpenAI-compatible hosted vendor (fallback)
OPENAI_API_KEY=
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini
OLLAMA_MODEL=llama3:8b
# Local Ollama: first request can take 10â€“30s while model loads (cold start)
LLM_PARSE_TIMEOUT_SECONDS=60
LLM_HOSTED_TIMEOUT_SECONDS=10
LLM_MAX_RETRIES=2
DEV_MODE=true
# Date-only and "today/tomorrow" logic use this timezone (e.g. America/Toronto)
ORG_TIMEZONE=America/Toronto

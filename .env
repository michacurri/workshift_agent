DATABASE_URL=postgresql+asyncpg://user:password@postgres:5432/scheduler
REDIS_URL=redis://redis:6379
LLM_PROVIDER=local
# Host Ollama (recommended for dev): run `ollama serve` on host.
# Note: to make Ollama reachable from Docker, you may need to bind it to 0.0.0.0
# (e.g. `OLLAMA_HOST=0.0.0.0:11434 ollama serve`).
OLLAMA_BASE_URL=http://host.docker.internal:11434
OPENAI_API_KEY=
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini
OLLAMA_MODEL=llama3:8b
# Local Ollama: first request can take 10â€“30s while model loads (cold start)
LLM_PARSE_TIMEOUT_SECONDS=60
LLM_HOSTED_TIMEOUT_SECONDS=10
LLM_MAX_RETRIES=2
DEV_MODE=true

